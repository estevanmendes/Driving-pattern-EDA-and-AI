{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lybraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay,confusion_matrix,precision_score,recall_score,roc_curve,roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from scipy.ndimage import gaussian_filter1d\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='data/test_motion_data.csv'\n",
    "test_path='data/train_motion_data.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(train_path)\n",
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Timestamp.unique().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 2 measurements per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Timestamp.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It proves that there is not a single timestamp that is replicated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Timestamp']=train_data['Timestamp'].astype('int')\n",
    "\n",
    "train_data['Class']=train_data['Class'].astype('category')\n",
    "train_data['class_code']=train_data['Class'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns=['AccX','AccY','AccZ','GyroX','GyroY','GyroZ']\n",
    "label_columns=['class_code','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of outliers, e.i., There is many measuments in the same timestamp that are desagreeing badly. However, it might be due to is amplitude. Then lets take a look in the boxplot normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gyroscope data is not desagreeing much. But, the accelaration that It massive desagreement in some measuments. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "diff=diff.reset_index().merge(train_data.drop_duplicates('Timestamp')[['Timestamp','Class']],right_on='Timestamp',left_on='index')\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(1,len(used_columns),figsize=(10,6))\n",
    "for column,ax in zip(used_columns,axes.flatten()):\n",
    "    sns.boxplot(data=diff,y=column,ax=ax,x='Class')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level of agreeament\n",
    "\n",
    "[1]Bland, J. Martin, and DouglasG Altman. \"Statistical methods for assessing agreement between two methods of clinical measurement.\" The lancet 327.8476 (1986): 307-310.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(measument1+measument2)*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "    ax.hlines(np.mean(diff[column])+2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "    ax.hlines(np.mean(diff[column])-2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, the data are quite odd and show significant different, Using [1] one might argui that both data agree.\n",
    "\n",
    "Also, With this it seems that there is not a specific range where the data desagree more.\n",
    "\n",
    "Since both measures may have differnt signs I will be taken the mean of its module, I wonder if there is some behaviour hidden due it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(np.abs(measument1)+np.abs(measument2))*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Acc_mod']=0.5*(train_data['AccX']**2+train_data['AccY']**2+train_data['AccZ']**2)\n",
    "var_columns.append('Acc_mod')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Class'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_seconds=set(train_data['Timestamp'].unique()).difference(set(range(train_data['Timestamp'].min(),train_data['Timestamp'].max()+1)))\n",
    "missing_seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we ensure the signal continuity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, it has been created some code for fixing some possible descontinuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avaible_seconds=train_data.Timestamp.unique()\n",
    "\n",
    "for second in missing_seconds:\n",
    "    new_index=len(train_data)+1\n",
    "    train_data.loc[new_index,['Timestamp']]=[second]\n",
    "    train_data.loc[new_index,var_columns]=[np.nan]*len(var_columns)\n",
    "   \n",
    "    for offset in range(1,11):\n",
    "        filled=False\n",
    "        if second-offset in avaible_seconds:\n",
    "            train_data.loc[new_index,label_columns]=train_data.drop_duplicates('Timestamp').set_index('Timestamp').loc[second-offset,label_columns]\n",
    "            filled=True\n",
    "            break\n",
    "        elif second+offset in avaible_seconds:\n",
    "            train_data.loc[new_index,label_columns]=train_data.drop_duplicates('Timestamp').set_index('Timestamp').loc[second+offset,label_columns]\n",
    "            filled=True\n",
    "            break\n",
    "\n",
    "    if not filled: print('Method did not work')\n",
    "    train_data.loc[new_index,label_columns]=[np.nan]*len(label_columns)\n",
    "    \n",
    "train_data=train_data.sort_values('Timestamp').reset_index(drop=True)\n",
    "train_data[var_columns]=train_data[var_columns].interpolate(method='polynomial',order=5)[var_columns]=train_data[var_columns].interpolate(method='linear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Class'].map({'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),1,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes.flatten(),var_columns):\n",
    "    grouped=train_data.groupby('Class')\n",
    "    color={'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}\n",
    "    for key,group in grouped:\n",
    "        group.plot(ax=ax,x='Timestamp',y=column,label=key,color=color[key])\n",
    "    ax.set_title(column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_analysis(data,fft_ax,psd_ax,prefix=''):\n",
    "    fft=np.fft.fft(data)\n",
    "    freq=np.fft.fftfreq(len(data),d=1)\n",
    "\n",
    "    fft_ax.plot(freq,fft.real,color='red',alpha=0.5,label='real')\n",
    "    fft_ax.plot(freq,fft.imag,color='blue',alpha=0.6,label='imag')\n",
    "    fft_ax.set_xlim(left=0)\n",
    "    fft_ax.set_title(prefix+' FFT')\n",
    "    fft_ax.legend()\n",
    "    psd=(1/len(data))*(fft.real**2+fft.imag**2)\n",
    "    psd_ax.set_title(prefix+' PSD')\n",
    "    psd_ax.plot(freq,psd,color='blue',alpha=0.7)\n",
    "    psd_ax.set_xlim(left=-0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),2,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    fourier_analysis(train_data[column],ax[0],ax[1],prefix=column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the fft of each class, since showed before, there are three continuos periods in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),6,figsize=(16,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    grouped=train_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=grouped.get_group(key)[column]\n",
    "        fourier_analysis(data,ax[groupd_index*2],ax[groupd_index*2+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram_analysis(data,ax,prefix=''):\n",
    "    f, t, Sxx = signal.spectrogram(data, 1)\n",
    "    im=ax.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title(prefix)\n",
    "    plt.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),4,figsize=(16,2*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    data=train_data[column]\n",
    "    spectogram_analysis(data,ax[0],f'All classes - {column}')\n",
    "    grouped=train_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=group[column]\n",
    "        spectogram_analysis(data,ax[groupd_index+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data - The previus analysys was replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(test_path)\n",
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Timestamp.unique().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 2 measurements per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Timestamp.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It proves that there is not a single timestamp that is replicated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Timestamp']=test_data['Timestamp'].astype('int')\n",
    "\n",
    "test_data['Class']=test_data['Class'].astype('category')\n",
    "test_data['class_code']=test_data['Class'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns=['AccX','AccY','AccZ','GyroX','GyroY','GyroZ']\n",
    "label_columns=['class_code','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "diff=diff.reset_index().merge(test_data.drop_duplicates('Timestamp')[['Timestamp','Class']],right_on='Timestamp',left_on='index')\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(1,len(used_columns),figsize=(10,6))\n",
    "for column,ax in zip(used_columns,axes.flatten()):\n",
    "    sns.boxplot(data=diff,y=column,ax=ax,x='Class')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level of agreeament\n",
    "\n",
    "[1]Bland, J. Martin, and DouglasG Altman. \"Statistical methods for assessing agreement between two methods of clinical measurement.\" The lancet 327.8476 (1986): 307-310.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(measument1+measument2)*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "    ax.hlines(np.mean(diff[column])+2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "    ax.hlines(np.mean(diff[column])-2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(np.abs(measument1)+np.abs(measument2))*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Acc_mod']=0.5*(test_data['AccX']**2+test_data['AccY']**2+test_data['AccZ']**2)\n",
    "var_columns.append('Acc_mod')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Class'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_seconds=set(test_data['Timestamp'].unique()).difference(set(range(test_data['Timestamp'].min(),test_data['Timestamp'].max()+1)))\n",
    "missing_seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we ensure the signal continuity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),1,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes.flatten(),var_columns):\n",
    "    grouped=test_data.groupby('Class')\n",
    "    color={'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}\n",
    "    for key,group in grouped:\n",
    "        group.plot(ax=ax,x='Timestamp',y=column,label=key,color=color[key])\n",
    "    ax.set_title(column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),2,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    fourier_analysis(test_data[column],ax[0],ax[1],prefix=column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the fft of each class, since showed before, there are three continuos periods in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),6,figsize=(16,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    grouped=test_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=grouped.get_group(key)[column]\n",
    "        fourier_analysis(data,ax[groupd_index*2],ax[groupd_index*2+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram_analysis(data,ax,prefix=''):\n",
    "    f, t, Sxx = signal.spectrogram(data, 1)\n",
    "    im=ax.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title(prefix)\n",
    "    plt.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),4,figsize=(16,2*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    data=test_data[column]\n",
    "    spectogram_analysis(data,ax[0],f'All classes - {column}')\n",
    "    grouped=test_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=group[column]\n",
    "        spectogram_analysis(data,ax[groupd_index+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing to aplly AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_numeric_categoric_data(df):\n",
    "\n",
    "    return df[var_columns],df[label_columns]\n",
    "\n",
    "\n",
    "def rooling_average(df:pd.DataFrame,window):\n",
    "    df_numeric,df_labels=split_numeric_categoric_data(df)\n",
    "    df_smoothed=df_numeric.rolling(window).mean()    \n",
    "    return pd.concat([df_smoothed,df_labels],axis=1)\n",
    "\n",
    "def exponential_smoothing(df:pd.DataFrame,alpha):\n",
    "    df_numeric,df_labels=split_numeric_categoric_data(df)\n",
    "    df_smoothed=df_numeric.ewm(alpha=alpha).mean()\n",
    "    return pd.concat([df_smoothed,df_labels],axis=1)\n",
    "   \n",
    "def gaussian_smoothing(df:pd.DataFrame,window,std):\n",
    "    df_numeric,df_labels=split_numeric_categoric_data(df)\n",
    "    df_smoothed=df_numeric.rolling(window,win_type='gaussian',center=True).mean(std=std)\n",
    "    return pd.concat([df_smoothed,df_labels],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of each smoothing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_dfs=[train_data.reset_index()\\\n",
    "              ,rooling_average(train_data,5).reset_index()\\\n",
    "              ,exponential_smoothing(train_data,0.1).reset_index()\\\n",
    "              ,gaussian_smoothing(train_data,5,3).reset_index()\\\n",
    "]\n",
    "denoise_methods_label=['Raw','Rolling Average','Exponential Smoothing','Gaussian filter']\n",
    "\n",
    "fig,axes=plt.subplots(len(var_columns),4,figsize=(16,3*len(var_columns)))\n",
    "\n",
    "for df,(index,label) in zip(denoised_dfs,enumerate(denoise_methods_label)):\n",
    "    for ax,column in zip (axes[:,index].flatten(),var_columns):\n",
    "        grouped=df.groupby('Class')\n",
    "        color={'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}\n",
    "        for key,group in grouped:\n",
    "            group.plot(ax=ax,x='index',y=column,label=key,color=color[key])\n",
    "        ax.set_title(column)\n",
    "    fig.text(.125-0.002*len(label)+0.25*index,1,label,fontsize=15,weight='bold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that will be built aims to find which data are related to agressive driving, then, it will became a binary classification problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data should be a sequence of seconds, since the acceleration of a given seconds does not provide enough information about the driving beahaviour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataLoader:\n",
    "    train_df:pd.DataFrame\n",
    "    test_df:pd.DataFrame\n",
    "    var_columns:list\n",
    "    class_column:list\n",
    "    seed=123    \n",
    "\n",
    "    def smoothing(self,method='rolling_average',**kwargs):        \n",
    "        \"\"\"\n",
    "        method: either 'gaussian','exponential','rolling_average'\n",
    "        \"\"\"\n",
    "        \n",
    "        if method=='exponential':\n",
    "            func=DataLoader.__exponential_smoothing\n",
    "        elif method=='gaussian':\n",
    "            func=DataLoader.__gaussian_smoothing\n",
    "        else:\n",
    "            func=DataLoader.__rooling_average\n",
    "        \n",
    "        self.train_df=self.__apply_smoothing(self.train_df,func,kwargs)\n",
    "        self.test_df=self.__apply_smoothing(self.test_df,func,kwargs)\n",
    "    \n",
    "    def __apply_smoothing(self,df,func,func_kwargs):\n",
    "        df_numeric,df_labels=df[self.var_columns],df[self.class_column]\n",
    "        df_numeric=func(df_numeric,**func_kwargs)\n",
    "        return pd.concat([df_numeric,df_labels],axis=1).dropna()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __rooling_average(df:pd.DataFrame,window):\n",
    "        df_smoothed=df.rolling(window).mean()\n",
    "        return df_smoothed\n",
    "\n",
    "    @staticmethod\n",
    "    def __exponential_smoothing(df:pd.DataFrame,alpha):\n",
    "        df_smoothed=df.ewm(alpha=alpha).mean()\n",
    "        return df_smoothed\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gaussian_smoothing(df:pd.DataFrame,window,std):\n",
    "        df_smoothed=df.rolling(window,win_type='gaussian',center=True).mean(std=std)\n",
    "        return df_smoothed\n",
    "\n",
    "    def __get_val_data(self):  \n",
    "        np.random.seed(self.seed)                 \n",
    "        X,Y=self.test_df[self.var_columns].values,self.test_df[self.class_column].values\n",
    "        self.Xtest,self.Xval,self.Ytest,self.Yval=train_test_split(X,Y,test_size=.5,stratify=Y)     \n",
    "\n",
    "    def __get_val_data_sequence(self):   \n",
    "        np.random.seed(self.seed)        \n",
    "        X,Y=self.XtestSequence,self.YtestSequence\n",
    "        self.XtestSequence,self.XvalSequence,self.YtestSequence,self.YvalSequence=train_test_split(X,Y,test_size=.5,stratify=Y)     \n",
    "    \n",
    "    def __create_sequences(self,df,sequence_size):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for key in df[self.class_column].unique():\n",
    "            \n",
    "            train_class=df[df[self.class_column]==key]    \n",
    "            for row_shift in range(sequence_size):\n",
    "                size=(len(train_class)-row_shift)//sequence_size*sequence_size\n",
    "                X_temp=train_class[var_columns].values[row_shift:size+row_shift].reshape(-1,sequence_size,len(var_columns))\n",
    "                Y_temp=np.array([key]*len(X_temp)).reshape(-1,1)\n",
    "                if len(X)!=0:\n",
    "                    X=np.vstack((X,X_temp))\n",
    "                    Y=np.vstack((Y,Y_temp))\n",
    "                else:\n",
    "                    X=X_temp\n",
    "                    Y=Y_temp\n",
    "        return X,Y\n",
    "\n",
    "    def get_train_val_test_arrays(self):\n",
    "        self.Xtrain=self.train_df[self.var_columns].values\n",
    "        self.Ytrain=self.train_df[self.class_column].values\n",
    "        self.__get_val_data()\n",
    "        return self.Xtrain,self.Ytrain,self.Xval,self.Yval,self.Xtest,self.Ytest\n",
    "    \n",
    "    def get_train_val_test_sequence(self):\n",
    "        self.__get_val_data_sequence()\n",
    "        return self.XtrainSequence,self.YtrianSequence,self.XvalSequence,self.YvalSequence,self.XtestSequence,self.Y_testSequence\n",
    "    \n",
    "    def to_sequecence(self,sequence_size):\n",
    "        self.XtrainSequence,self.YtrainSequence=self.__create_sequences(self.train_df,sequence_size)\n",
    "        self.XtestSequence,self.YtestSequence=self.__create_sequences(self.test_df,sequence_size)\n",
    "        \n",
    "    \n",
    "    def to_binary_classes(self,map):\n",
    "        self.train_df[self.class_column[0]]=self.train_df[self.class_column[0]].map(map)\n",
    "        self.test_df[self.class_column[0]]=self.test_df[self.class_column[0]].map(map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(train_data.copy(),test_data.copy(),var_columns,['class_code'])\n",
    "map={0:1,2:0,1:0}\n",
    "dataloader.to_binary_classes(map)\n",
    "dataloader.smoothing('gaussian',window=5,std=3)\n",
    "X_train,Y_train,X_val,Y_val,X_test,Y_test=dataloader.get_train_val_test_arrays()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[label_columns].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[label_columns].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(Y_val,return_counts=True),np.unique(Y_test,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainSequence,Y_trainSequence,X_valSequence,Y_valSequence,X_testSequence,Y_testSequence=dataloader.get_train_val_test_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainSequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_trainSequence,return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel():\n",
    "    def __init__(self,model,Xtrain,Ytrain,scaler=False,random_state=123,**kwargs):\n",
    "        self.random_state=random_state\n",
    "        if scaler:\n",
    "            self.model=make_pipeline(StandardScaler(),model(**kwargs))\n",
    "        else:\n",
    "            self.model=model(**kwargs)\n",
    "        self.Xtrain=Xtrain\n",
    "        self.Ytrain=Ytrain\n",
    "        self._get_label_proportion(Ytrain,prefix='train')\n",
    "        \n",
    "    \n",
    "    def fit(self):      \n",
    "        np.random.seed(self.random_state)  \n",
    "        self.model.fit(self.Xtrain,self.Ytrain)\n",
    "\n",
    "    def evaluate(self,Xtest,Ytest):\n",
    "        self._get_label_proportion(Ytest,prefix='test')\n",
    "        self.pred=self.model.predict(Xtest)\n",
    "        self._get_accuracy(Ytest)\n",
    "        self._get_confusion_matrix(Ytest)\n",
    "        self._get_class_report(Ytest)\n",
    "\n",
    "    def _get_confusion_matrix(self,Ytest):\n",
    "        cm = confusion_matrix(Ytest,self.pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "    \n",
    "    def _get_class_report(self,Ytest):\n",
    "        report=classification_report(Ytest,self.pred)\n",
    "        print(report)\n",
    "\n",
    "    def _get_accuracy(self,Ytest):\n",
    "        accuracy=accuracy_score(Ytest,self.pred)\n",
    "        print(f'The accuracy observed was {accuracy*100:.0f} %')\n",
    "    \n",
    "    def _get_label_proportion(self,Y,prefix=''):\n",
    "        true_rate=(sum(Y==1)/len(Y))[0]\n",
    "        print(f'{prefix}: The dataset has {true_rate*100:.0f}% of positive')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=ClassificationModel(LogisticRegression,X_train,Y_train,n_jobs=-1)\n",
    "LR.fit()\n",
    "LR.evaluate(X_val,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=ClassificationModel(LogisticRegression,X_train,Y_train,scaler=True)\n",
    "LR.fit()\n",
    "LR.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummiest classifier would guess everything negative and would have the same result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model=ClassificationModel(SVC,X_train,Y_train,scaler=True)\n",
    "SVC_model.fit()\n",
    "SVC_model.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=ClassificationModel(RandomForestClassifier,X_train,Y_train,n_jobs=-1)\n",
    "RF.fit()\n",
    "RF.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB=ClassificationModel(GaussianNB,X_train,Y_train)\n",
    "MB.fit()\n",
    "MB.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB=ClassificationModel(XGBClassifier,X_train,Y_train)\n",
    "XGB.fit()\n",
    "XGB.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the models have performed well, as it was mentioned before, our base line shloud be ~69%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod\n",
    "class DLModel():\n",
    "\n",
    "    def __init__(self,Xtrain,Ytrain,Xval,Yval,Xtest,Ytest):\n",
    "        self.Xtrain=Xtrain\n",
    "        self.Ytrain=Ytrain\n",
    "        self.Xval=Xval\n",
    "        self.Yval=Yval\n",
    "        self.Xtest=Xtest\n",
    "        self.Ytest=Ytest\n",
    "        self.training_history=[]\n",
    "        self.model=self.model_compiler()\n",
    "    \n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def model_compiler(self):\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def train_params(self)->dict:\n",
    "        pass   \n",
    "\n",
    "    def train(self,seed=123,verbose=0):\n",
    "        np.random.seed(seed)\n",
    "        history=self.model.fit(self.Xtrain,self.Ytrain,validation_data=(self.Xval,self.Yval),verbose=verbose,**self.training_params())\n",
    "        self.training_history.append(history)     \n",
    "\n",
    "    def test(self):\n",
    "        threshold=0.5\n",
    "        self.predictions=self.model.predict(self.Xtest)\n",
    "        self.binary_predictions=(self.predictions>threshold).astype(int)\n",
    "        test_accuracy=accuracy_score(self.Ytest,self.binary_predictions)        \n",
    "        test_precision=precision_score(self.Ytest,self.binary_predictions)\n",
    "        test_recall=recall_score(self.Ytest,self.binary_predictions)\n",
    "        auc=roc_auc_score(self.Ytest,self.binary_predictions)\n",
    "        print(f'Test accuracy : {test_accuracy*100:.0f} %')\n",
    "        print(f'Test precision : {test_precision*100:.0f} %')\n",
    "        print(f'Test recall : {test_recall*100:.0f} %')\n",
    "        print(f'Model AUC : {auc*100:.0f} %')\n",
    "        self.confusion_matrix_plot()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __unpack_data_history(history_record,key):\n",
    "        key=DLModel.__get_metric_key_by_prefix(key,history_record)\n",
    "        metric=[]\n",
    "        for train_data in history_record:\n",
    "            metric.extend(train_data.history[key])\n",
    "        \n",
    "        return metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_metric_key_by_prefix(key,history_record):\n",
    "        key_in_history=''\n",
    "        if 'val' in key:\n",
    "            for metric in history_record[-1].history.keys():\n",
    "                if key in metric:\n",
    "                    key_in_history=metric\n",
    "                    break\n",
    "        else:\n",
    "            for metric in history_record[-1].history.keys():\n",
    "                if key in metric and 'val' not in metric:\n",
    "                    key_in_history=metric\n",
    "                    break\n",
    "\n",
    "        return key_in_history\n",
    "\n",
    "    def loss_plot(self,ax=None):\n",
    "        if not ax:\n",
    "            loss=DLModel.__unpack_data_history(self.training_history,'loss')\n",
    "            plt.title('loss')\n",
    "            plt.plot(loss)\n",
    "            plt.xlabel('epochs')\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def accuracy_plot(self,ax=None):\n",
    "        if not ax:        \n",
    "            accuracy=DLModel.__unpack_data_history(self.training_history,'binary_accuracy')\n",
    "            val_accuracy=DLModel.__unpack_data_history(self.training_history,'val_binary_accuracy')\n",
    "            plt.title('Accuracy')\n",
    "            plt.plot(accuracy,label='train')\n",
    "            plt.plot(val_accuracy,label='val')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epochs')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def precision_recall_plot(self,ax=None):\n",
    "        if not ax:\n",
    "            precision=DLModel.__unpack_data_history(self.training_history,'precision')\n",
    "            recall=DLModel.__unpack_data_history(self.training_history,'recall')\n",
    "            val_precision=DLModel.__unpack_data_history(self.training_history,'val_precision')\n",
    "            val_recall=DLModel.__unpack_data_history(self.training_history,'val_recall')\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.title('Precision')\n",
    "            plt.plot(precision,label='Train')\n",
    "            plt.plot(val_precision,label='Val') \n",
    "            plt.xlabel('epochs')\n",
    "            plt.legend()\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.title('Recall')\n",
    "            plt.plot(recall,label='Train')\n",
    "            plt.plot(val_recall,label='Val') \n",
    "            plt.xlabel('epochs')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def confusion_matrix_plot(self):\n",
    "        if hasattr(self, 'predictions'):\n",
    "            cm = confusion_matrix(self.Ytest,self.binary_predictions)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot()\n",
    "        else:\n",
    "            print('Test method should be called before')\n",
    "\n",
    "    def plot_all_training_metrics(self):\n",
    "        self.loss_plot()\n",
    "        self.accuracy_plot()\n",
    "        self.precision_recall_plot()\n",
    "\n",
    "    def multiple_seeds_analysis(self,seeds:list):\n",
    "        pass\n",
    "          \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "\n",
    "    def model_compiler(self):\n",
    "        MLP_model=keras.Sequential(\n",
    "                [\n",
    "                \n",
    "                    keras.Input(shape=(7,)),\n",
    "                    keras.layers.Dense(10,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                    keras.layers.Dense(10,activation='tanh'),\n",
    "                    keras.layers.Dropout(0.4),    \n",
    "                    keras.layers.Dense(6,activation='tanh'),\n",
    "                    keras.layers.Dense(6,activation='tanh'),\n",
    "                    keras.layers.Dense(1,activation='sigmoid')\n",
    "                ]\n",
    "                )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "        return MLP_model\n",
    "\n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_1(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_1(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=321)\n",
    "# model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(10,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(10,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(10,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_2(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_2(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=123)\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_3(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(2,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_3(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_3(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=123)\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_4(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='selu', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='selu'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='selu'),\n",
    "                keras.layers.Dense(3,activation='selu'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='selu'),\n",
    "                keras.layers.Dense(4,activation='selu'),\n",
    "                keras.layers.Dense(2,activation='selu'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_4(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_5(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(2,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_5(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_6(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_6(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_6(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=321)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cac705f4ee24bcb73acdd8b9241d3bce8cf7d46d1c48a1c8f0ce576ed75e5a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
