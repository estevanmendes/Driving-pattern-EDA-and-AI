{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lybraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report,accuracy_score,ConfusionMatrixDisplay,confusion_matrix,precision_score,recall_score,roc_curve,roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow import keras\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='data/test_motion_data.csv'\n",
    "test_path='data/train_motion_data.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(train_path)\n",
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Timestamp.unique().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 2 measurements per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.Timestamp.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It proves that there is not a single timestamp that is replicated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Timestamp']=train_data['Timestamp'].astype('int')\n",
    "\n",
    "train_data['Class']=train_data['Class'].astype('category')\n",
    "train_data['class_code']=train_data['Class'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns=['AccX','AccY','AccZ','GyroX','GyroY','GyroZ']\n",
    "label_columns=['class_code','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of outliers, e.i., There is many measuments in the same timestamp that are desagreeing badly. However, it might be due to is amplitude. Then lets take a look in the boxplot normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gyroscope data is not desagreeing much. But, the accelaration that It massive desagreement in some measuments. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "diff=diff.reset_index().merge(train_data.drop_duplicates('Timestamp')[['Timestamp','Class']],right_on='Timestamp',left_on='index')\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(1,len(used_columns),figsize=(10,6))\n",
    "for column,ax in zip(used_columns,axes.flatten()):\n",
    "    sns.boxplot(data=diff,y=column,ax=ax,x='Class')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level of agreeament\n",
    "\n",
    "[1]Bland, J. Martin, and DouglasG Altman. \"Statistical methods for assessing agreement between two methods of clinical measurement.\" The lancet 327.8476 (1986): 307-310.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(measument1+measument2)*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "    ax.hlines(np.mean(diff[column])+2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "    ax.hlines(np.mean(diff[column])-2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, the data are quite odd and show significant different, Using [1] one might argui that both data agree.\n",
    "\n",
    "Also, With this it seems that there is not a specific range where the data desagree more.\n",
    "\n",
    "Since both measures may have differnt signs I will be taken the mean of its module, I wonder if there is some behaviour hidden due it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(train_data.Timestamp.value_counts()>1).index\n",
    "measument1=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=train_data.set_index(train_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(np.abs(measument1)+np.abs(measument2))*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Acc_mod']=0.5*(train_data['AccX']**2+train_data['AccY']**2+train_data['AccZ']**2)\n",
    "var_columns.append('Acc_mod')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Class'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_seconds=set(train_data['Timestamp'].unique()).difference(set(range(train_data['Timestamp'].min(),train_data['Timestamp'].max()+1)))\n",
    "missing_seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we ensure the signal continuity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, it has been created some code for fixing some possible descontinuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avaible_seconds=train_data.Timestamp.unique()\n",
    "\n",
    "for second in missing_seconds:\n",
    "    new_index=len(train_data)+1\n",
    "    train_data.loc[new_index,['Timestamp']]=[second]\n",
    "    train_data.loc[new_index,var_columns]=[np.nan]*len(var_columns)\n",
    "   \n",
    "    for offset in range(1,11):\n",
    "        filled=False\n",
    "        if second-offset in avaible_seconds:\n",
    "            train_data.loc[new_index,label_columns]=train_data.drop_duplicates('Timestamp').set_index('Timestamp').loc[second-offset,label_columns]\n",
    "            filled=True\n",
    "            break\n",
    "        elif second+offset in avaible_seconds:\n",
    "            train_data.loc[new_index,label_columns]=train_data.drop_duplicates('Timestamp').set_index('Timestamp').loc[second+offset,label_columns]\n",
    "            filled=True\n",
    "            break\n",
    "\n",
    "    if not filled: print('Method did not work')\n",
    "    train_data.loc[new_index,label_columns]=[np.nan]*len(label_columns)\n",
    "    \n",
    "train_data=train_data.sort_values('Timestamp').reset_index(drop=True)\n",
    "train_data[var_columns]=train_data[var_columns].interpolate(method='polynomial',order=5)[var_columns]=train_data[var_columns].interpolate(method='linear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Class'].map({'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),1,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes.flatten(),var_columns):\n",
    "    grouped=train_data.groupby('Class')\n",
    "    color={'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}\n",
    "    for key,group in grouped:\n",
    "        group.plot(ax=ax,x='Timestamp',y=column,label=key,color=color[key])\n",
    "    ax.set_title(column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_analysis(data,fft_ax,psd_ax,prefix=''):\n",
    "    fft=np.fft.fft(data)\n",
    "    freq=np.fft.fftfreq(len(data),d=1)\n",
    "\n",
    "    fft_ax.plot(freq,fft.real,color='red',alpha=0.5,label='real')\n",
    "    fft_ax.plot(freq,fft.imag,color='blue',alpha=0.6,label='imag')\n",
    "    fft_ax.set_xlim(left=0)\n",
    "    fft_ax.set_title(prefix+' FFT')\n",
    "    fft_ax.legend()\n",
    "    psd=(1/len(data))*(fft.real**2+fft.imag**2)\n",
    "    psd_ax.set_title(prefix+' PSD')\n",
    "    psd_ax.plot(freq,psd,color='blue',alpha=0.7)\n",
    "    psd_ax.set_xlim(left=-0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),2,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    fourier_analysis(train_data[column],ax[0],ax[1],prefix=column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the fft of each class, since showed before, there are three continuos periods in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),6,figsize=(16,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    grouped=train_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=grouped.get_group(key)[column]\n",
    "        fourier_analysis(data,ax[groupd_index*2],ax[groupd_index*2+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram_analysis(data,ax,prefix=''):\n",
    "    f, t, Sxx = signal.spectrogram(data, 1)\n",
    "    im=ax.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title(prefix)\n",
    "    plt.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),4,figsize=(16,2*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    data=train_data[column]\n",
    "    spectogram_analysis(data,ax[0],f'All classes - {column}')\n",
    "    grouped=train_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=group[column]\n",
    "        spectogram_analysis(data,ax[groupd_index+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data - The previus analysys was replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(test_path)\n",
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Timestamp.unique().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 2 measurements per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Timestamp.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It proves that there is not a single timestamp that is replicated many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Timestamp']=test_data['Timestamp'].astype('int')\n",
    "\n",
    "test_data['Class']=test_data['Class'].astype('category')\n",
    "test_data['class_code']=test_data['Class'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns=['AccX','AccY','AccZ','GyroX','GyroY','GyroZ']\n",
    "label_columns=['class_code','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "fig,axes=plt.subplots(1,len(var_columns),figsize=(10,6))\n",
    "for column,ax in zip(var_columns,axes.flatten()):    \n",
    "    sns.boxplot(data=diff,y=column,ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=(measument1-measument2)/0.5*(measument2+measument1)\n",
    "diff=diff.reset_index().merge(test_data.drop_duplicates('Timestamp')[['Timestamp','Class']],right_on='Timestamp',left_on='index')\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(1,len(used_columns),figsize=(10,6))\n",
    "for column,ax in zip(used_columns,axes.flatten()):\n",
    "    sns.boxplot(data=diff,y=column,ax=ax,x='Class')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level of agreeament\n",
    "\n",
    "[1]Bland, J. Martin, and DouglasG Altman. \"Statistical methods for assessing agreement between two methods of clinical measurement.\" The lancet 327.8476 (1986): 307-310.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(measument1+measument2)*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "    ax.hlines(np.mean(diff[column])+2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "    ax.hlines(np.mean(diff[column])-2*np.std(diff[column]),mean[used_columns].min().min(),mean[used_columns].max().max(),linestyles='dashed',color='black')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=(test_data.Timestamp.value_counts()>1).index\n",
    "measument1=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='first',subset='Timestamp').loc[index,var_columns]\n",
    "measument2=test_data.set_index(test_data.Timestamp).drop_duplicates(keep='last',subset='Timestamp').loc[index,var_columns]\n",
    "diff=measument1-measument2\n",
    "mean=(np.abs(measument1)+np.abs(measument2))*0.5\n",
    "used_columns=[column for column in var_columns if 'Acc' in column]\n",
    "fig,axes=plt.subplots(len(used_columns),1,figsize=(10,6),sharex=True)\n",
    "for column,ax in zip(used_columns,axes.flatten()):    \n",
    "    ax.scatter(x=mean[column],y=diff[column])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Acc_mod']=0.5*(test_data['AccX']**2+test_data['AccY']**2+test_data['AccZ']**2)\n",
    "var_columns.append('Acc_mod')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Class'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_seconds=set(test_data['Timestamp'].unique()).difference(set(range(test_data['Timestamp'].min(),test_data['Timestamp'].max()+1)))\n",
    "missing_seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we ensure the signal continuity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data as a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),1,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes.flatten(),var_columns):\n",
    "    grouped=test_data.groupby('Class')\n",
    "    color={'SLOW':'green','NORMAL':'blue','AGGRESSIVE':'red'}\n",
    "    for key,group in grouped:\n",
    "        group.plot(ax=ax,x='Timestamp',y=column,label=key,color=color[key])\n",
    "    ax.set_title(column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),2,figsize=(8,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    fourier_analysis(test_data[column],ax[0],ax[1],prefix=column)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the fft of each class, since showed before, there are three continuos periods in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),6,figsize=(16,3*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    grouped=test_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=grouped.get_group(key)[column]\n",
    "        fourier_analysis(data,ax[groupd_index*2],ax[groupd_index*2+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram_analysis(data,ax,prefix=''):\n",
    "    f, t, Sxx = signal.spectrogram(data, 1)\n",
    "    im=ax.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title(prefix)\n",
    "    plt.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(len(var_columns),4,figsize=(16,2*len(var_columns)))\n",
    "\n",
    "for ax,column in zip (axes,var_columns):\n",
    "    data=test_data[column]\n",
    "    spectogram_analysis(data,ax[0],f'All classes - {column}')\n",
    "    grouped=test_data.groupby('Class')\n",
    "    for groupd_index,(key,group) in enumerate(grouped):\n",
    "        data=group[column]\n",
    "        spectogram_analysis(data,ax[groupd_index+1],prefix=f'{key} - {column}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing to aplly AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that will built aims to find which data are related to agressive driving, then, it will became a binary classification problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data should be a sequence of seconds, since the acceleration of a given seconds does not provide enough information about the driving beahaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[label_columns].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[label_columns].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map={0:1,2:0,1:0}\n",
    "train_data.class_code=train_data.class_code.map(map)\n",
    "test_data.class_code=test_data.class_code.map(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['class_code'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['class_code'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(sequence_size,class_column,classes_key:list,df):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for key in classes_key:\n",
    "        \n",
    "        train_class=df[df[class_column]==key]    \n",
    "        size=len(train_class)//sequence_size*sequence_size\n",
    "        X_temp=train_class[var_columns.copy()].values[:size].reshape(-1,sequence_size,len(var_columns))\n",
    "        Y_temp=np.array([key]*len(X_temp)).reshape(-1,1)\n",
    "        if len(X)!=0:\n",
    "            X=np.vstack((X,X_temp))\n",
    "            Y=np.vstack((Y,Y_temp))\n",
    "        else:\n",
    "            X=X_temp\n",
    "            Y=Y_temp\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequece,Y_train_sequence=create_sequences(3,'class_code',[0,1,2],train_data)\n",
    "X_test_sequence,Y_test_sequence=create_sequences(3,'class_code',[0,1,2],test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "X_test_sequence,X_val_sequence,Y_test_sequence,Y_val_sequence=train_test_split(X_test_sequence,Y_test_sequence,test_size=0.5,stratify=Y_test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data[var_columns]\n",
    "Y_train=train_data['class_code']\n",
    "X_test=test_data[var_columns]\n",
    "Y_test=test_data['class_code']\n",
    "X_test,X_val,Y_test,Y_val=train_test_split(X_test,Y_test,test_size=0.5,stratify=Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y_train==1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel():\n",
    "    def __init__(self,model,Xtrain,Ytrain,scaler=False,random_state=123,**kwargs):\n",
    "        self.random_state=random_state\n",
    "        if scaler:\n",
    "            self.model=make_pipeline(StandardScaler(),model(**kwargs))\n",
    "        else:\n",
    "            self.model=model(**kwargs)\n",
    "        self.Xtrain=Xtrain\n",
    "        self.Ytrain=Ytrain\n",
    "        self._get_label_proportion(Ytrain,prefix='train')\n",
    "        \n",
    "    \n",
    "    def fit(self):      \n",
    "        np.random.seed(self.random_state)  \n",
    "        self.model.fit(self.Xtrain,self.Ytrain)\n",
    "\n",
    "    def evaluate(self,Xtest,Ytest):\n",
    "        self._get_label_proportion(Ytest,prefix='test')\n",
    "        self.pred=self.model.predict(X_test)\n",
    "        self._get_accuracy(Ytest)\n",
    "        self._get_confusion_matrix(Ytest)\n",
    "        self._get_class_report(Ytest)\n",
    "\n",
    "    def _get_confusion_matrix(self,Ytest):\n",
    "        cm = confusion_matrix(Ytest,self.pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "    \n",
    "    def _get_class_report(self,Ytest):\n",
    "        report=classification_report(Ytest,self.pred)\n",
    "        print(report)\n",
    "\n",
    "    def _get_accuracy(self,Ytest):\n",
    "        accuracy=accuracy_score(Ytest,self.pred)\n",
    "        print(f'The accuracy observed was {accuracy*100:.0f} %')\n",
    "    \n",
    "    def _get_label_proportion(self,Y,prefix=''):\n",
    "        true_rate=sum(Y==1)/len(Y)\n",
    "        print(f'{prefix}: The dataset has {true_rate*100:.0f}% of positive')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=ClassificationModel(LogisticRegression,X_train,Y_train,n_jobs=-1)\n",
    "LR.fit()\n",
    "LR.evaluate(X_val,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=ClassificationModel(LogisticRegression,X_train,Y_train,scaler=True)\n",
    "LR.fit()\n",
    "LR.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummiest classifier would guess everything negative and would have the same result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model=ClassificationModel(SVC,X_train,Y_train,scaler=True)\n",
    "SVC_model.fit()\n",
    "SVC_model.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=ClassificationModel(RandomForestClassifier,X_train,Y_train,n_jobs=-1)\n",
    "RF.fit()\n",
    "RF.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB=ClassificationModel(GaussianNB,X_train,Y_train)\n",
    "MB.fit()\n",
    "MB.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB=ClassificationModel(XGBClassifier,X_train,Y_train)\n",
    "XGB.fit()\n",
    "XGB.evaluate(X_val,Y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the models have performed well, as it was mentioned before, our base line shloud be ~69%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractclassmethod\n",
    "class DLModel():\n",
    "\n",
    "    def __init__(self,Xtrain,Ytrain,Xval,Yval,Xtest,Ytest):\n",
    "        self.Xtrain=Xtrain\n",
    "        self.Ytrain=Ytrain\n",
    "        self.Xval=Xval\n",
    "        self.Yval=Yval\n",
    "        self.Xtest=Xtest\n",
    "        self.Ytest=Ytest\n",
    "        self.training_history=[]\n",
    "        self.model=self.model_compiler()\n",
    "    \n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def model_compiler(self):\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def train_params(self)->dict:\n",
    "        pass   \n",
    "\n",
    "    def train(self,seed=123,verbose=0):\n",
    "        np.random.seed(seed)\n",
    "        history=self.model.fit(self.Xtrain,self.Ytrain,validation_data=(self.Xval,self.Yval),verbose=verbose,**self.training_params())\n",
    "        self.training_history.append(history)     \n",
    "\n",
    "    def test(self):\n",
    "        threshold=0.5\n",
    "        self.predictions=self.model.predict(self.Xtest)\n",
    "        self.binary_predictions=(self.predictions>threshold).astype(int)\n",
    "        test_accuracy=accuracy_score(self.Ytest,self.binary_predictions)        \n",
    "        test_precision=precision_score(self.Ytest,self.binary_predictions)\n",
    "        test_recall=recall_score(self.Ytest,self.binary_predictions)\n",
    "        auc=roc_auc_score(self.Ytest,self.binary_predictions)\n",
    "        print(f'Test accuracy : {test_accuracy*100:.0f} %')\n",
    "        print(f'Test precision : {test_precision*100:.0f} %')\n",
    "        print(f'Test recall : {test_recall*100:.0f} %')\n",
    "        print(f'Model AUC : {auc*100:.0f} %')\n",
    "        self.confusion_matrix_plot()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __unpack_data_history(history_record,key):\n",
    "        key=DLModel.__get_metric_key_by_prefix(key,history_record)\n",
    "        metric=[]\n",
    "        for train_data in history_record:\n",
    "            metric.extend(train_data.history[key])\n",
    "        \n",
    "        return metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_metric_key_by_prefix(key,history_record):\n",
    "        key_in_history=''\n",
    "        if 'val' in key:\n",
    "            for metric in history_record[-1].history.keys():\n",
    "                if key in metric:\n",
    "                    key_in_history=metric\n",
    "                    break\n",
    "        else:\n",
    "            for metric in history_record[-1].history.keys():\n",
    "                if key in metric and 'val' not in metric:\n",
    "                    key_in_history=metric\n",
    "                    break\n",
    "\n",
    "        return key_in_history\n",
    "\n",
    "    def loss_plot(self,ax=None):\n",
    "        if not ax:\n",
    "            loss=DLModel.__unpack_data_history(self.training_history,'loss')\n",
    "            plt.title('loss')\n",
    "            plt.plot(loss)\n",
    "            plt.xlabel('epochs')\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def accuracy_plot(self,ax=None):\n",
    "        if not ax:        \n",
    "            accuracy=DLModel.__unpack_data_history(self.training_history,'binary_accuracy')\n",
    "            val_accuracy=DLModel.__unpack_data_history(self.training_history,'val_binary_accuracy')\n",
    "            plt.title('Accuracy')\n",
    "            plt.plot(accuracy,label='train')\n",
    "            plt.plot(val_accuracy,label='val')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epochs')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def precision_recall_plot(self,ax=None):\n",
    "        if not ax:\n",
    "            precision=DLModel.__unpack_data_history(self.training_history,'precision')\n",
    "            recall=DLModel.__unpack_data_history(self.training_history,'recall')\n",
    "            val_precision=DLModel.__unpack_data_history(self.training_history,'val_precision')\n",
    "            val_recall=DLModel.__unpack_data_history(self.training_history,'val_recall')\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.title('Precision')\n",
    "            plt.plot(precision,label='Train')\n",
    "            plt.plot(val_precision,label='Val') \n",
    "            plt.xlabel('epochs')\n",
    "            plt.legend()\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.title('Recall')\n",
    "            plt.plot(recall,label='Train')\n",
    "            plt.plot(val_recall,label='Val') \n",
    "            plt.xlabel('epochs')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def confusion_matrix_plot(self):\n",
    "        if hasattr(self, 'predictions'):\n",
    "            cm = confusion_matrix(self.Ytest,self.binary_predictions)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot()\n",
    "        else:\n",
    "            print('Test method should be called before')\n",
    "\n",
    "    def plot_all_training_metrics(self):\n",
    "        self.loss_plot()\n",
    "        self.accuracy_plot()\n",
    "        self.precision_recall_plot()\n",
    "\n",
    "    def multiple_seeds_analysis(self,seeds:list):\n",
    "        pass\n",
    "          \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "\n",
    "    def model_compiler(self):\n",
    "        MLP_model=keras.Sequential(\n",
    "                [\n",
    "                \n",
    "                    keras.Input(shape=(7,)),\n",
    "                    keras.layers.Dense(10,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                    keras.layers.Dense(10,activation='tanh'),\n",
    "                    keras.layers.Dropout(0.4),    \n",
    "                    keras.layers.Dense(6,activation='tanh'),\n",
    "                    keras.layers.Dense(6,activation='tanh'),\n",
    "                    keras.layers.Dense(1,activation='sigmoid')\n",
    "                ]\n",
    "                )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "        return MLP_model\n",
    "\n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_1(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_1(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=321)\n",
    "# model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(10,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(10,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(10,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_2(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_2(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=123)\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_3(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(2,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_3(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_3(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=123)\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_4(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='selu', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='selu'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='selu'),\n",
    "                keras.layers.Dense(3,activation='selu'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='selu'),\n",
    "                keras.layers.Dense(4,activation='selu'),\n",
    "                keras.layers.Dense(2,activation='selu'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_4(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_5(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dense(5,activation='tanh'),\n",
    "                keras.layers.Dropout(0.4),    \n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dropout(0.7),  \n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(6,activation='tanh'),\n",
    "                keras.layers.Dense(4,activation='tanh'),\n",
    "                keras.layers.Dense(2,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_5(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_6(DLModel):\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def model_compiler(self):        \n",
    "        MLP_model=keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(7,)),            \n",
    "                keras.layers.Dense(5,activation='tanh', kernel_initializer=\"glorot_uniform\"),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(3,activation='tanh'),\n",
    "                keras.layers.Dense(1,activation='sigmoid')\n",
    "            ]\n",
    "            )\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps=10000,decay_rate=0.9)\n",
    "        model_loss=keras.losses.BinaryCrossentropy()\n",
    "        model_optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.Precision(),keras.metrics.Recall()]\n",
    "        MLP_model.compile(loss=model_loss,optimizer=model_optimizer,metrics=model_metrics)\n",
    "\n",
    "        return MLP_model\n",
    "    \n",
    "    def training_params(self):\n",
    "        epochs=300\n",
    "        batch_size=32\n",
    "        callback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        return {'callbacks':[callback,tensorboard_callback],'epochs':epochs,'batch_size':batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_6(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.architecture\n",
    "model.train(seed=123)\n",
    "model.plot_all_training_metrics()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_6(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "model.train(seed=321)\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cac705f4ee24bcb73acdd8b9241d3bce8cf7d46d1c48a1c8f0ce576ed75e5a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
